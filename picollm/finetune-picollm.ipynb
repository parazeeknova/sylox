{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Fine-tuning Qwen 2.5 0.5B Instruct on KonCode Dataset for improving Coding Reasoning","metadata":{}},{"cell_type":"code","source":"# Fine-tuning Qwen2.5-0.5B-Instruct on KodCode Dataset for Coding Reasoning\n# This cell sets up the environment for fine-tuning the Qwen2.5-0.5B-Instruct model on the KodCode dataset\n!pip install -q datasets transformers evaluate accelerate torch bitsandbytes peft\n\n# pytorch - a deep learning framework that provides tensor computations with GPU acceleration\n# datasets - Offers efficient data loading, processing, and caching, Provides tools for working with ML datasets\n# bitsandbytes - Enables memory-efficient training of large models & Reduces memory usage by quantizing model weights\n# peft (Parameter-Efficient Fine-Tuning) - Implements efficient fine-tuning methods like LoRA\n# transformers - Contains pre-trained language models and tools to work with them \n\n# Set up the environment and check CUDA availability\nimport torch\nimport os\nimport random \nfrom datasets import load_dataset\nimport bitsandbytes as bnb\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_from_disk\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:20:30.875786Z","iopub.execute_input":"2025-03-20T18:20:30.876088Z","iopub.status.idle":"2025-03-20T18:21:00.762776Z","shell.execute_reply.started":"2025-03-20T18:20:30.876066Z","shell.execute_reply":"2025-03-20T18:21:00.761866Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCUDA available: True\nCUDA device: Tesla P100-PCIE-16GB\nCUDA version: 12.1\nGPU memory: 17.06 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"This cell prepares the environment for fine-tuning Qwen2.5-0.5B-Instruct on the KodCode dataset. It installs required libraries, imports essential modules, and checks GPU availability. The setup enables QLoRA fine-tuning with 4-bit quantization, allowing the model to learn coding skills while minimizing memory usage on Colab's T4 GPU.","metadata":{}},{"cell_type":"code","source":"def load_and_examine_dataset(dataset_name, split=\"train\"):\n    \"\"\"\n    Load a dataset from Hugging Face and examine its format\n\n    Args:\n        dataset_name: Name of the dataset on Hugging Face Hub\n        split: Dataset split to load (default: \"train\")\n        \n    Returns:\n        The loaded dataset\n    \"\"\"\n    # Load the dataset\n    dataset = load_dataset(dataset_name, split=split)\n    \n    # Print dataset info\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Number of examples: {len(dataset)}\")\n    print(f\"Dataset features: {dataset.features}\")\n    \n    # Print a sample example\n    print(\"\\nSample example:\")\n    sample = dataset[0]\n    for key, value in sample.items():\n        if isinstance(value, str) and len(value) > 100:\n            print(f\"{key}: {value[:100]}... (truncated)\")\n        else:\n            print(f\"{key}: {value}\")\n    \n    return dataset\n\ndataset = load_and_examine_dataset(\"KodCode/KodCode-V1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:21:00.764033Z","iopub.execute_input":"2025-03-20T18:21:00.764724Z","iopub.status.idle":"2025-03-20T18:21:41.960176Z","shell.execute_reply.started":"2025-03-20T18:21:00.764700Z","shell.execute_reply":"2025-03-20T18:21:41.959293Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.67k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42786793bc2b4896a7560f40273a517a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00015.parquet:   0%|          | 0.00/198M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67020dda27dd40fc9e71417893ae7cd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00015.parquet:   0%|          | 0.00/118M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba59b263e3ca471b8c6fb82bc22a57d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00015.parquet:   0%|          | 0.00/154M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a5013df2c443a49b5d6d3435de7ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00015.parquet:   0%|          | 0.00/239M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"409332f40cea4d759803a66d9b000bb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00015.parquet:   0%|          | 0.00/210M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34c95a64f66b46bcac31c7d8efddb5bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00015.parquet:   0%|          | 0.00/221M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6e6fe753cf4656a5c101e192000417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00015.parquet:   0%|          | 0.00/88.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"695f59797b1244e193ff71a765cd6eab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00007-of-00015.parquet:   0%|          | 0.00/138M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4f14860bb244539bde16b5b538beb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00008-of-00015.parquet:   0%|          | 0.00/181M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e57b19e52f4e7ca93d46e4625bf478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00009-of-00015.parquet:   0%|          | 0.00/146M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5b41670e2644404b5dd76fbb4a37bed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00010-of-00015.parquet:   0%|          | 0.00/159M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a86d5db836574522b75749f79a153b54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00011-of-00015.parquet:   0%|          | 0.00/208M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8f9a6accda44cdbe34a4421424b9b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00012-of-00015.parquet:   0%|          | 0.00/254M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acafbbf6f3dc46fd9d17e4255052d5d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00013-of-00015.parquet:   0%|          | 0.00/177M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2abf2c8da98b446b8ae3188c11a33d00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00014-of-00015.parquet:   0%|          | 0.00/132M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09401713def435a97dec16d6a3baef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"use_with_caution-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbcf53c987ae4d86b6b61eb93df0af15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/484097 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afdc6d2a93664f2899fcada5bd15236f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating use_with_caution split:   0%|          | 0/3335 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"346ce601779c4cf8bcee4614f8e15906"}},"metadata":{}},{"name":"stdout","text":"Dataset: KodCode/KodCode-V1\nNumber of examples: 484097\nDataset features: {'version': Value(dtype='string', id=None), 'style': Value(dtype='string', id=None), 'subset': Value(dtype='string', id=None), 'question_id': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'solution': Value(dtype='string', id=None), 'test': Value(dtype='string', id=None), 'test_info': [{'docstring': Value(dtype='string', id=None), 'function_declaration': Value(dtype='string', id=None), 'function_name': Value(dtype='string', id=None), 'parameter_list': Value(dtype='string', id=None)}], 'gpt_pass_sequence': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'gpt_pass_trial_num': Value(dtype='int64', id=None), 'gpt_difficulty': Value(dtype='string', id=None), 'gpt_pass_percentage': Value(dtype='float64', id=None), 'trials': {'trial_gpt4o_0': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_1': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_2': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_3': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_4': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_5': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_6': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_7': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_8': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}, 'trial_gpt4o_9': {'file_source': Value(dtype='string', id=None), 'solution_code': Value(dtype='string', id=None), 'test_code': Value(dtype='string', id=None), 'test_coverage': Value(dtype='float64', id=None), 'test_result': Value(dtype='string', id=None)}}, 'chosen_trial': Value(dtype='string', id=None), 'metadata': {'original_instruction': Value(dtype='string', id=None), 'prompt_id': Value(dtype='string', id=None), 'row_id': Value(dtype='int64', id=None), 'seed_ids': Value(dtype='string', id=None)}, 'benchmark_similarity': Value(dtype='float64', id=None), 'benchmark_instruction': Value(dtype='string', id=None), 'benchmark_task_id': Value(dtype='string', id=None), 'filter_reason': Value(dtype='string', id=None)}\n\nSample example:\nversion: v1.0\nstyle: instruct\nsubset: Leetcode\nquestion_id: Leetcode_3_I\nquestion: Given a list of integers `nums`, find the maximum product of any two distinct elements in the list. ... (truncated)\nsolution: def max_product(nums):\n    \"\"\"\n    Returns the maximum product of any two distinct elements in the l... (truncated)\ntest: from solution import max_product\n\ndef test_max_product_positive_numbers():\n    assert max_product([1... (truncated)\ntest_info: [{'docstring': 'Returns the maximum product of any two distinct elements in the list.', 'function_declaration': 'def max_product(nums):', 'function_name': 'max_product', 'parameter_list': '(nums)'}]\ngpt_pass_sequence: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\ngpt_pass_trial_num: 1\ngpt_difficulty: hard\ngpt_pass_percentage: 0.1\ntrials: {'trial_gpt4o_0': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_0_results_combined.jsonl', 'solution_code': 'def max_product(nums):\\n    \"\"\"\\n    Returns the maximum product of any two distinct elements in the list.\\n    \\n    :param nums: List of integers\\n    :return: Maximum product of any two distinct elements\\n    \"\"\"\\n    if len(nums) < 2:\\n        raise ValueError(\"List must contain at least two elements\")\\n    \\n    nums.sort()\\n    return max(nums[0] * nums[1], nums[-1] * nums[-2])', 'test_code': 'def test_max_product_with_positive_numbers():\\n    assert max_product([5, 3, 9]) == 45\\n\\ndef test_max_product_with_negatives():\\n    assert max_product([5, 3, -1, 9, -7]) == 45\\n\\ndef test_max_product_with_all_negatives():\\n    assert max_product([-5, -3, -9, -1]) == -3\\n\\ndef test_max_product_with_mix_of_positives_and_negatives():\\n    assert max_product([-10, -20, 5, 2]) == 200\\n\\ndef test_max_product_with_two_elements():\\n    assert max_product([1, 10]) == 10\\n\\ndef test_max_product_with_large_numbers():\\n    assert max_product([100000, 200000, 300000]) == 60000000000\\n\\ndef test_max_product_with_only_two_negatives():\\n    assert max_product([-1000, -2000]) == 2000000\\n\\ndef test_max_product_with_duplicates():\\n    assert max_product([3, 3, 3, -1]) == 9\\n\\ndef test_max_product_raises_value_error_with_too_few_elements():\\n    try:\\n        max_product([5])\\n    except ValueError as e:\\n        assert str(e) == \"List must contain at least two elements\"', 'test_coverage': None, 'test_result': 'Fail'}, 'trial_gpt4o_1': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_1_results_combined.jsonl', 'solution_code': 'def max_product(nums):\\n    \"\"\"\\n    Given a list of integers nums, find the maximum product of any two distinct elements in the list.\\n    \\n    Parameters:\\n    nums (list): List of integers.\\n\\n    Returns:\\n    int: Maximum product of any two distinct elements.\\n    \"\"\"\\n    nums.sort()\\n    return max(nums[0]*nums[1], nums[-1]*nums[-2])', 'test_code': 'from solution import max_product\\n\\ndef test_positive_numbers():\\n    assert max_product([2, 4, 6, 8]) == 48\\n\\ndef test_with_negatives():\\n    assert max_product([5, 3, -1, 9, -7]) == 45\\n    assert max_product([-10, -20, 5, 2]) == 200\\n\\ndef test_all_negative_numbers():\\n    assert max_product([-5, -3, -2, -4]) == 6\\n\\ndef test_including_zero():\\n    assert max_product([1, 2, 3, 0]) == 6\\n    assert max_product([0, -3, -2, -1, 1]) == 3\\n\\ndef test_two_elements():\\n    assert max_product([3, 5]) == 15\\n    assert max_product([-4, 2]) == -8\\n\\ndef test_large_numbers():\\n    assert max_product([1000, 2000, -2000]) == 4000000', 'test_coverage': None, 'test_result': 'Fail'}, 'trial_gpt4o_2': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_2_results_combined.jsonl', 'solution_code': 'def max_product(nums):\\n    \"\"\"\\n    Returns the maximum product of any two distinct elements in the list nums.\\n    \"\"\"\\n    if len(nums) < 2:\\n        raise ValueError(\"The list must contain at least two elements.\")\\n    \\n    nums.sort()\\n    \\n    # The maximum product will be the maximum of the product of the two largest numbers \\n    # or the product of the two smallest numbers (in case they\\'re negative and their product is positive)\\n    max1 = nums[-1] * nums[-2]\\n    max2 = nums[0] * nums[1]\\n    \\n    return max(max1, max2)', 'test_code': 'from solution import max_product\\n\\ndef test_max_product_all_positive():\\n    assert max_product([5, 3, 1, 9, 7]) == 63\\n\\ndef test_max_product_include_negatives():\\n    assert max_product([5, 3, -1, 9, -7]) == 45\\n\\ndef test_max_product_all_negatives():\\n    assert max_product([-5, -3, -1, -9, -7]) == -3\\n\\ndef test_max_product_mixed():\\n    assert max_product([-10, -10, 5, 2]) == 100\\n\\ndef test_max_product_minimum_elements():\\n    assert max_product([3, 4]) == 12\\n\\ndef test_max_product_zero_included():\\n    assert max_product([0, 4, 3, 2]) == 12\\n\\ndef test_max_product_all_zeros():\\n    assert max_product([0, 0, 0, 0]) == 0\\n\\ndef test_max_product_two_largest_same_value():\\n    assert max_product([3, 3, 3, 3]) == 9\\n\\ndef test_max_product_raises_error_for_too_few_elements():\\n    try:\\n        max_product([1])\\n    except ValueError as e:\\n        assert str(e) == \"The list must contain at least two elements\"\\n    else:\\n        assert False, \"Expected ValueError, but no exception was raised\"', 'test_coverage': None, 'test_result': 'Fail'}, 'trial_gpt4o_3': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_3_results_combined.jsonl', 'solution_code': 'def max_product(nums):\\n    \"\"\"\\n    Returns the maximum product of any two distinct elements in the list.\\n    \"\"\"\\n    if len(nums) < 2:\\n        raise ValueError(\"The list must contain at least two elements.\")\\n    \\n    nums.sort()\\n    \\n    # The maximum product can be from the two largest positive numbers\\n    # Or from the two smallest (most negative) numbers, because multiplying two\\n    # negative numbers yields a positive number\\n    return max(nums[-1] * nums[-2], nums[0] * nums[1])', 'test_code': 'from solution import max_product\\n\\ndef test_max_product_positive_numbers():\\n    nums = [1, 2, 3, 4, 5]\\n    assert max_product(nums) == 20  # 4 * 5\\n\\ndef test_max_product_including_negatives():\\n    nums = [5, 3, -1, 9, -7]\\n    assert max_product(nums) == 45  # 5 * 9\\n\\ndef test_max_product_only_negatives():\\n    nums = [-10, -3, -1, -4, -5]\\n    assert max_product(nums) == 15  # -3 * -5\\n\\ndef test_max_product_mixed_sign_numbers():\\n    nums = [-10, -3, 1, 2, 10]\\n    assert max_product(nums) == 100  # -10 * -10 (or 10 * 10)\\n\\ndef test_max_product_edge_case_two_elements():\\n    nums = [1, 2]\\n    assert max_product(nums) == 2  # 1 * 2\\n\\ndef test_max_product_edge_case_two_negatives():\\n    nums = [-1, -2]\\n    assert max_product(nums) == 2  # -1 * -2\\n\\ndef test_max_product_identical_elements():\\n    nums = [2, 2, 2, 2]\\n    assert max_product(nums) == 4  # 2 * 2\\n\\ndef test_max_product_large_numbers():\\n    nums = [1000, 100000, 1000000, -1000, -100000]\\n    assert max_product(nums) == 100000000  # 100000 * 1000000', 'test_coverage': None, 'test_result': 'Fail'}, 'trial_gpt4o_4': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_4_results_combined.jsonl', 'solution_code': 'def max_product(nums):\\n    \"\"\"\\n    Finds the maximum product of any two distinct elements in the list.\\n    \"\"\"\\n    nums.sort()\\n    return max(nums[-1] * nums[-2], nums[0] * nums[1])', 'test_code': 'from solution import max_product\\n\\ndef test_max_product_positive_numbers():\\n    assert max_product([1, 5, 3, 2, 8]) == 40  # 8 * 5\\n\\ndef test_max_product_with_negatives():\\n    assert max_product([5, 3, -1, 9, -7]) == 45  # 5 * 9\\n\\ndef test_max_product_all_negatives():\\n    assert max_product([-1, -2, -3, -4]) == -1  # -1 * -2\\n\\ndef test_max_product_mixture():\\n    assert max_product([-10, 1, 3, -5, 2]) == 6  # 2 * 3\\n\\ndef test_max_product_zeros():\\n    assert max_product([0, 0, 2, 3]) == 6  # 2 * 3\\n    assert max_product([0, 0, -2, -3]) == 0  # 0 * -2\\n\\ndef test_max_product_multiple_same_max():\\n    assert max_product([1, 2, 2, 2, 3, 3]) == 9  # 3 * 3', 'test_coverage': None, 'test_result': 'Fail'}, 'trial_gpt4o_5': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_5_results_combined.jsonl', 'solution_code': 'def max_product_of_two(nums):\\n    \"\"\"\\n    Returns the maximum product of any two distinct elements in the list.\\n    \"\"\"\\n    if len(nums) < 2:\\n        raise ValueError(\"At least two elements are required\")\\n    \\n    nums.sort()\\n    return max(nums[0] * nums[1], nums[-1] * nums[-2])', 'test_code': 'import pytest\\nfrom solution import max_product_of_two\\n\\ndef test_example():\\n    nums = [5, 3, -1, 9, -7]\\n    assert max_product_of_two(nums) == 45\\n\\ndef test_all_positive():\\n    nums = [1, 2, 3, 4, 5]\\n    assert max_product_of_two(nums) == 20\\n\\ndef test_all_negative():\\n    nums = [-1, -2, -3, -4, -5]\\n    assert max_product_of_two(nums) == -1\\n\\ndef test_positive_and_negative():\\n    nums = [-10, -3, 5, 6, -2]\\n    assert max_product_of_two(nums) == 30\\n\\ndef test_including_zeros():\\n    nums = [0, 1, 2, 3, 4]\\n    assert max_product_of_two(nums) == 12\\n    nums = [0, 0, -1, -2, -3]\\n    assert max_product_of_two(nums) == 0\\n\\ndef test_single_product_case():\\n    nums = [1, 0, -5, -2]\\n    assert max_product_of_two(nums) == 10\\n\\ndef test_raise_error_for_insufficient_elements():\\n    nums = [1]\\n    with pytest.raises(ValueError):\\n        max_product_of_two(nums)', 'test_coverage': None, 'test_result': 'Fail'}, 'trial_gpt4o_6': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_6_results_combined.jsonl', 'solution_code': 'def max_product(nums):\\n    \"\"\"\\n    Returns the maximum product of any two distinct elements in the list nums.\\n    \"\"\"\\n    if len(nums) < 2:\\n        return None  # Not enough elements to form a pair\\n\\n    nums.sort()\\n    # The maximum product can be formed by:\\n    # 1. The product of the two largest numbers\\n    # 2. The product of the two smallest numbers (if they are both negative)\\n    return max(nums[0] * nums[1], nums[-1] * nums[-2])', 'test_code': 'from solution import max_product\\n\\ndef test_max_product_typical_case():\\n    assert max_product([5, 3, -1, 9, -7]) == 45\\n\\ndef test_max_product_single_positive_pair():\\n    assert max_product([1, 2]) == 2\\n\\ndef test_max_product_with_negatives():\\n    assert max_product([-10, -20, 5, 3]) == 200\\n\\ndef test_max_product_all_negatives():\\n    assert max_product([-10, -20, -5, -3]) == 200\\n\\ndef test_max_product_single_pair():\\n    assert max_product([-1, -2]) == 2\\n\\ndef test_max_product_long_list():\\n    assert max_product([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, -11, -12, -13, -14, 100]) == 1000\\n\\ndef test_max_product_not_enough_elements():\\n    assert max_product([5]) == None\\n\\ndef test_max_product_mixed_negatives_and_positives():\\n    assert max_product([3, 4, -2, 5, -6]) == 12', 'test_coverage': None, 'test_result': 'Fail'}, 'trial_gpt4o_7': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_7_results_combined.jsonl', 'solution_code': 'def max_product(nums):\\n    \"\"\"\\n    Returns the maximum product of any two distinct elements in the list.\\n    \"\"\"\\n    # Sort the list\\n    nums.sort()\\n    \\n    # The maximum product could be from the two highest values or two lowest values (in case they are negative)\\n    return max(nums[-1] * nums[-2], nums[0] * nums[1])', 'test_code': 'from solution import max_product\\n\\ndef test_max_product_positive_numbers():\\n    assert max_product([1, 5, 3, 7, 9]) == 63  # 7 * 9 = 63\\n\\ndef test_max_product_with_negatives():\\n    assert max_product([-1, -3, -5, 2, 4]) == 15  # -3 * -5 = 15\\n\\ndef test_max_product_mixed_signs():\\n    assert max_product([5, 3, -1, 9, -7]) == 45  # 5 * 9 = 45\\n\\ndef test_max_product_two_numbers():\\n    assert max_product([2, 3]) == 6  # 2 * 3 = 6\\n\\ndef test_max_product_single_zero():\\n    assert max_product([0, 5, 10]) == 50  # 5 * 10 = 50\\n\\ndef test_max_product_zeroes():\\n    assert max_product([0, 0, 2, 3]) == 6  # 2 * 3 = 6\\n\\ndef test_max_product_large_numbers():\\n    assert max_product([1000, 3000, 5000, 2000]) == 15000000  # 3000 * 5000 = 15000000\\n\\ndef test_max_product_identical_elements():\\n    assert max_product([5, 5, 5, 5]) == 25  # 5 * 5 = 25', 'test_coverage': 100.0, 'test_result': 'Pass'}, 'trial_gpt4o_8': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_8_results_combined.jsonl', 'solution_code': 'def maximum_product_of_two(nums):\\n    \"\"\"\\n    Returns the maximum product of any two distinct elements in the list.\\n    \\n    :param nums: List of integers\\n    :return: Maximum product of any two distinct elements\\n    \"\"\"\\n    nums.sort()\\n    # The maximum product can be from the two largest numbers or the two smallest (if they are negative)\\n    candidate1 = nums[-1] * nums[-2]  # Largest and the second largest\\n    candidate2 = nums[0] * nums[1]    # Smallest and the second smallest (both negative)\\n\\n    return max(candidate1, candidate2)', 'test_code': 'from solution import maximum_product_of_two\\n\\ndef test_positive_numbers():\\n    assert maximum_product_of_two([5, 3, 9, 7]) == 63  # 9 * 7\\n\\ndef test_all_negative_numbers():\\n    assert maximum_product_of_two([-5, -3, -9, -7]) == 35  # (-5) * (-7)\\n\\ndef test_mixed_sign_numbers():\\n    assert maximum_product_of_two([5, 3, -1, 9, -7]) == 45  # 5 * 9\\n\\ndef test_mixed_sign_numbers_with_zeros():\\n    assert maximum_product_of_two([5, 3, 0, -1, 9, -7]) == 45  # 5 * 9\\n\\ndef test_large_value_numbers():\\n    assert maximum_product_of_two([1000, 2000, 3000, -1000, -2000, -3000]) == 9000000  # 3000 * 3000\\n\\ndef test_tie_cases():\\n    assert maximum_product_of_two([1, 2, 1, 2]) == 4  # 2 * 2\\n    assert maximum_product_of_two([-1, -2, -1, -2]) == 4  # (-2) * (-2)\\n    \\ndef test_minimum_length():\\n    assert maximum_product_of_two([1, 2]) == 2  # Only two numbers, 1 * 2', 'test_coverage': None, 'test_result': 'Fail'}, 'trial_gpt4o_9': {'file_source': 'CodeGen_instruction_50000_combined_sanitized_prepared_9_results_combined.jsonl', 'solution_code': 'def max_product(nums):\\n    \"\"\"\\n    Returns the maximum product of any two distinct elements in the list.\\n    \"\"\"\\n    if len(nums) < 2:\\n        raise ValueError(\"The list must contain at least two elements.\")\\n    \\n    # Initialize the two largest and two smallest values\\n    max1 = max2 = float(\\'-inf\\')\\n    min1 = min2 = float(\\'inf\\')\\n    \\n    for num in nums:\\n        if num > max1:\\n            max2 = max1\\n            max1 = num\\n        elif num > max2:\\n            max2 = num\\n        \\n        if num < min1:\\n            min2 = min1\\n            min1 = num\\n        elif num < min2:\\n            min2 = num\\n    \\n    return max(max1 * max2, min1 * min2)', 'test_code': 'from solution import max_product\\n\\ndef test_max_product_with_positive_numbers():\\n    assert max_product([1, 2, 3, 4, 5]) == 20  # 4 * 5\\n\\ndef test_max_product_with_mixed_numbers():\\n    assert max_product([5, 3, -1, 9, -7]) == 45  # 5 * 9\\n\\ndef test_max_product_with_two_elements():\\n    assert max_product([7, -3]) == -21  # 7 * -3\\n\\ndef test_max_product_with_all_negative_numbers():\\n    assert max_product([-10, -3, -5, -2]) == -6  # -2 * -3\\n\\ndef test_max_product_with_duplicates():\\n    assert max_product([1, 2, 3, 2]) == 6  # 2 * 3\\n\\ndef test_max_product_with_zero():\\n    assert max_product([0, -1, 2, 3]) == 6  # 2 * 3', 'test_coverage': None, 'test_result': 'Fail'}}\nchosen_trial: trial_gpt4o_7\nmetadata: {'original_instruction': '', 'prompt_id': '0000000003', 'row_id': 3, 'seed_ids': \"['1028', '54', '1009']\"}\nbenchmark_similarity: 0.7460421919822693\nbenchmark_instruction: \nbenchmark_task_id: \nfilter_reason: \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load the Qwen tokenizer & Load the dataset\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ndataset = load_dataset(\"KodCode/KodCode-V1\", split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:21:41.961333Z","iopub.execute_input":"2025-03-20T18:21:41.961645Z","iopub.status.idle":"2025-03-20T18:21:45.425090Z","shell.execute_reply.started":"2025-03-20T18:21:41.961621Z","shell.execute_reply":"2025-03-20T18:21:45.424036Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd17ae4ecaf4f04af35110fba8bdc3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a179ca0fb30e4d71870bdd1c95244f82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c3b2410a0a4105b457d5a20d7f4cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f5488a1d6f49658f04e247605c2aae"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"This code preprocesses the KodCode dataset for fine-tuning the Qwen2.5-0.5B-Instruct model. It formats each example as an instruction-response pair using the chat template format required by Qwen models. The process includes extracting coding problems and solutions, formatting them with special tokens, tokenizing the text, and splitting the dataset into training and validation sets. The code also saves samples of the processed data for inspection and stores the tokenized datasets to disk for efficient training.","metadata":{}},{"cell_type":"code","source":"# Function to format examples for instruction fine-tuning\ndef format_instruction(example):\n    # Extract the question and solution\n    question = example[\"question\"]\n    solution = example[\"solution\"]\n    \n    # Format as instruction-response pair\n    instruction = f\"Write a function to solve the following coding problem:\\n\\n{question}\"\n    response = solution\n    \n    # Create the formatted prompt\n    formatted_text = f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\"\n    \n    return {\"text\": formatted_text}\n\n# Apply the formatting function to the dataset\nformatted_dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n# Split the dataset into train and validation sets (95% train, 5% validation)\nformatted_dataset = formatted_dataset.train_test_split(test_size=0.05, seed=42)\n\n# Function to tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=2048,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n\n# Tokenize the dataset\ntokenized_dataset = {}\ntokenized_dataset[\"train\"] = formatted_dataset[\"train\"].map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"text\"]\n)\ntokenized_dataset[\"validation\"] = formatted_dataset[\"test\"].map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"text\"]\n)\n\n# Save a sample of the processed dataset for inspection\nsample_size = min(5, len(tokenized_dataset[\"train\"]))\nsample_indices = random.sample(range(len(tokenized_dataset[\"train\"])), sample_size)\nsample_data = [tokenized_dataset[\"train\"][i] for i in sample_indices]\n\nprint(f\"Processed {len(tokenized_dataset['train'])} training examples\")\nprint(f\"Processed {len(tokenized_dataset['validation'])} validation examples\")\nprint(f\"Sample of processed data (decoded):\")\nfor i, sample in enumerate(sample_data):\n    print(f\"\\nSample {i+1}:\")\n    decoded_text = tokenizer.decode(sample[\"input_ids\"])\n    print(decoded_text[:500] + \"...\" if len(decoded_text) > 500 else decoded_text)\n\n# Save the processed dataset\ntokenized_dataset[\"train\"].save_to_disk(\"./processed_dataset_train\")\ntokenized_dataset[\"validation\"].save_to_disk(\"./processed_dataset_validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:21:45.426583Z","iopub.execute_input":"2025-03-20T18:21:45.426822Z","iopub.status.idle":"2025-03-20T18:34:16.393763Z","shell.execute_reply.started":"2025-03-20T18:21:45.426801Z","shell.execute_reply":"2025-03-20T18:34:16.392072Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/484097 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1041597c059243dca0e9dec16cda3127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/459892 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e67569996214754a494272c5ef312c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24205 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6bdbe8460614ad1a7a6b0c09580ab6d"}},"metadata":{}},{"name":"stdout","text":"Processed 459892 training examples\nProcessed 24205 validation examples\nSample of processed data (decoded):\n\nSample 1:\n<|im_start|>user\nWrite a function to solve the following coding problem:\n\n### Context\nBob is building a system that requires maintaining and retrieving user session data. He has a list of session intervals with start and end timestamps for various users and needs to determine the number of active sessions at any given time.\n\n### Task\nWrite a function `find_maximum_active_sessions` that takes a list of session intervals and returns the maximum number of active sessions at any point in time. Each ...\n\nSample 2:\n<|im_start|>user\nWrite a function to solve the following coding problem:\n\nGiven a list of `n` integers representing the heights of vertical lines on a histogram, where the width of each bar is 1, return _the area of the largest rectangle that can be formed within the histogram._ The rectangle must be formed using consecutive bars in the histogram.\n\n### Format: Generate an executable Python function to solve the given problem. The function should read input from `stdin` and write the output to `s...\n\nSample 3:\n<|im_start|>user\nWrite a function to solve the following coding problem:\n\nGiven a list of strings, create a program that finds the longest common prefix among them. If no common prefix exists, return an empty string.\n\n-----Input:-----\n- First line contains an integer $T$, the number of test cases. Then the test cases follow.\n- The first line of each test case contains an integer $N$, the number of strings.\n- Next $N$ lines each contain one string.\n\n-----Output:-----\nFor each test case, output th...\n\nSample 4:\n<|im_start|>user\nWrite a function to solve the following coding problem:\n\nHow can I efficiently find the longest word in a given string using Python?<|im_end|>\n<|im_start|>assistant\ndef find_longest_word(string):\n    \"\"\"\n    Returns the longest word in the given string.\n    If there are multiple words of the same length, it returns the first one.\n    \"\"\"\n    words = string.split()\n    if not words:\n        return \"\"\n    longest_word = max(words, key=len)\n    return longest_word<|im_end|><|im_end...\n\nSample 5:\n<|im_start|>user\nWrite a function to solve the following coding problem:\n\nYou are tasked with creating a Python module for distributing a given set of source files. Your goal is to automatically generate the necessary built distribution files for various platforms and formats using the Distutils utility.\n\n**Task:**\n\n1. Implement a function `create_built_distribution(config)` that generates built distribution files based on the given configuration.\n\n**Function Signature:**\n```python\ndef create_bu...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/10 shards):   0%|          | 0/459892 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce090263f044d2d960198970563b735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/24205 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015c5ece5503463f95ed01f691f29704"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"This code sets up the Qwen2.5-0.5B-Instruct model with 4-bit quantization and LoRA for efficient fine-tuning. It configures the quantization parameters, loads the model and tokenizer, applies LoRA with optimized parameters, and prepares the dataset for training. The code also reduces the dataset size to speed up the training process.","metadata":{}},{"cell_type":"code","source":"# Set up quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\n# Load the model with quantization\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    use_cache=False\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Prepare the model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Define LoRA configuration - reduced rank for faster training\nlora_config = LoraConfig(\n    r=8,                      # Reduced rank for faster training\n    lora_alpha=16,            # Adjusted alpha\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Reduced target modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\nprint(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n\n# Load the processed dataset\ntrain_dataset = load_from_disk(\"./processed_dataset_train\")\neval_dataset = load_from_disk(\"./processed_dataset_validation\")\n\n# Use a smaller subset for faster training\ntrain_size = min(5000, len(train_dataset))\neval_size = min(500, len(eval_dataset))\ntrain_dataset = train_dataset.select(range(train_size))\neval_dataset = eval_dataset.select(range(eval_size))\n\nprint(f\"Training on {train_size} examples, evaluating on {eval_size} examples\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:34:16.397906Z","iopub.execute_input":"2025-03-20T18:34:16.400006Z","iopub.status.idle":"2025-03-20T18:34:50.822026Z","shell.execute_reply.started":"2025-03-20T18:34:16.399966Z","shell.execute_reply":"2025-03-20T18:34:50.821306Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f4d77c602e242db8e0709cacf5571fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd276d56a62949ccb59dfb3268ea1f2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c2fb29ae3ef486088a87fb3721aecfa"}},"metadata":{}},{"name":"stdout","text":"trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\nTrainable parameters: None\nTraining on 5000 examples, evaluating on 500 examples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"This code sets up the training arguments and executes the fine-tuning process for the Qwen2.5-0.5B-Instruct model. It configures training parameters optimized for memory efficiency, trains the model using the Trainer API, and saves the fine-tuned model to disk.","metadata":{}},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen_qlora_output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=50,\n    save_steps=200,\n    save_total_limit=2,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    fp16=True,\n    warmup_steps=100,\n    optim=\"adamw_torch\",\n    report_to=\"tensorboard\",\n    gradient_checkpointing=True,\n    remove_unused_columns=False,\n    max_grad_norm=0.3,\n    lr_scheduler_type=\"cosine\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\nmodel.save_pretrained(\"./qwen_qlora_final\")\ntokenizer.save_pretrained(\"./qwen_qlora_final\")\n\nprint(\"Training completed and model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:34:50.822768Z","iopub.execute_input":"2025-03-20T18:34:50.822981Z","iopub.status.idle":"2025-03-20T18:41:12.792839Z","shell.execute_reply.started":"2025-03-20T18:34:50.822962Z","shell.execute_reply":"2025-03-20T18:41:12.791105Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-c86f7ef980d6>:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='21' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  21/1875 05:50 < 9:29:50, 0.05 it/s, Epoch 0.03/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c86f7ef980d6>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./qwen_qlora_final\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./qwen_qlora_final\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3651\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3652\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_accepts_loss_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3655\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3707\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3709\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3710\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1720\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1166\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m    884\u001b[0m                     \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    624\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0;31m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;31m# The reason is that in some cases, an error can occur that backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0mabsmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdequantize_blockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0mabsmax\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m             \u001b[0mabsmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6}]}