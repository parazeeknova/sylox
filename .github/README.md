# Sylox - Syntactic Logic Exploration
Enhancing Code Reasoning through Fine-Tuning Pre-Trained Language Models

## Overview

Sylox is a research project focused on improving code reasoning capabilities in small to medium-scale language models through specialized fine-tuning. The project explores the enhancement of multiple pre-trained models using custom coding datasets to optimize their performance in software development tasks.

## Models

- TinyLlama 1.1B
- LLaMA 3.2 1B
- Qwen 2.5 1.5B & 0.5B
- DeepSeek R1 1.5B

## Objectives

- Enhance code comprehension and interpretation capabilities
- Improve logical reasoning in programming contexts
- Optimize problem-solving accuracy
- Evaluate the effectiveness of fine-tuning on smaller scale models
- Provide insights for domain-specific LLM optimization

## Methodology

1. **Pre-training Assessment**: Benchmark initial model performance
2. **Fine-tuning**: Train models on specialized coding datasets
3. **Evaluation**: Measure improvements using standardized metrics
4. **Analysis**: Compare pre and post fine-tuning results

## Evaluation Metrics

- Code comprehension accuracy
- Logical reasoning capabilities
- Problem-solving effectiveness
- Cross-language understanding
- Code generation quality