{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune tinyLlama 1.1B Model Training with unsloth library on RTX 3060 GPU\n",
    "\n",
    "This notebook implements optimized training of TinyLlama 1.1B on hardware with limited VRAM (6GB RTX 3060), uses unsloth library and HuggingFace accelerate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device properties: {torch.cuda.get_device_properties(0)}\")\n",
    "    print(f\"Current GPU memory usage: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"CPU count: {os.cpu_count()}\")\n",
    "print(f\"Available memory: {psutil.virtual_memory().available / (1024 * 1024 * 1024):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE = 2\n",
    "MAX_TRAIN_SAMPLES = 10000\n",
    "MAX_VAL_SAMPLES = 500\n",
    "GRADIENT_ACCUMULATION = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with streaming to reduce memory usage\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"KodCode/KodCode-V1\", streaming=False)\n",
    "print(f\"Dataset: {ds}\")\n",
    "\n",
    "def format_instruction(example):\n",
    "    return {\n",
    "        \"instruction\": example[\"question\"],\n",
    "        \"input\": \"\",\n",
    "        \"output\": example[\"solution\"]\n",
    "    }\n",
    "\n",
    "formatted_ds = ds[\"train\"].map(format_instruction)\n",
    "formatted_ds = formatted_ds.shuffle(seed=42)\n",
    "train_ds = formatted_ds.select(range(min(MAX_TRAIN_SAMPLES, len(formatted_ds))))\n",
    "val_ds = formatted_ds.select(range(MAX_TRAIN_SAMPLES, min(MAX_TRAIN_SAMPLES + MAX_VAL_SAMPLES, len(formatted_ds))))\n",
    "\n",
    "print(f\"Training examples: {len(train_ds)}\")\n",
    "print(f\"Validation examples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Tokenize function with dynamic padding for better memory efficiency\n",
    "def tokenize_function(examples):\n",
    "    formatted_texts = [f\"<s>[INST] {inst} [/INST] {out}</s>\" \n",
    "                      for inst, out in zip(examples[\"instruction\"], examples[\"output\"])]\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        formatted_texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets in smaller batches\n",
    "print(\"Tokenizing training dataset...\")\n",
    "tokenized_train_ds = train_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation dataset...\")\n",
    "tokenized_val_ds = val_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=val_ds.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare model with optimized settings\n",
    "print(\"Loading model...\")\n",
    "model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "# Configure LoRA parameters - use very small LoRA for memory efficiency\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Print model parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Percentage of trainable parameters: {trainable_params/total_params*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training with optimized settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kodcode_llama_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    torch_compile=False,\n",
    "    # Additional memory optimizations\n",
    "    max_grad_norm=1.0,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Monitor memory usage before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated before training: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved before training: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model_save_path = \"./kodcode_llama_model_final\"\n",
    "trainer.save_model(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final memory usage report\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Final GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Final GPU memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "After training, you can test the model with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "peft_model_path = \"./kodcode_llama_model_final\"\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "\n",
    "# Test the model\n",
    "prompt = \"<s>[INST] Write a Python function to calculate the factorial of a number [/INST]\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
